{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# L9-2 Carousel Personalization in Music Streaming Apps with Contextual Bandits\n",
    "## Abstract\n",
    "The artile written by W. Bendada, et al. presents a multi-armed bandit system that does not use common recommendation algorithms, as it only uses reinforcement learning in order to reccomend items to users.\n",
    "\n",
    "The work domain is music playlist carousels for users, hence the RL algorithm attemps to learn a probability $p_{ui} \\in [0,1]$ for item $i \\in [1, ...K]$  and user $u \\in [1,...N]$. In order to reduce the parameters (originally $K \\times N$), users are either represented by a contextual vector of length D, or belong to one of Q clusters. Being there K items to reccomend, this gives us either a $K \\times D$ parameters, or a $K \\times Q$ parameters.\n",
    "\n",
    "The authors also present a Cascade-based uptade method for their framework, but since it's a relatively specific to domain scenario we will not explain it here.\n",
    "Finally, the authors use various (actually, a lot) of RL algorithms to test, which include:\n",
    "\n",
    " - random : baseline, selects a random item\n",
    " - $\\epsilon$-greedy-seg: two versions, seg-explore ($\\epsilon = 0.1$) and seg-exploit ($\\epsilon = 0.01$)\n",
    " - etc-seg: Random until all items have been sampled X times.\n",
    " - kl-ucb-seg: Upper Confidence Bound strategy.\n",
    " - ts-seg: Thompson Sampling strategy\n",
    " - ts-lin: extension of Thompson sampling tailored to contextual vectors (instead of learning $p_{ui}$ this algorithm learns a parameter $\\theta _i$)\n",
    "\n",
    "The experiments uses a dataset realesed by a music streaming company, Deezer. \n",
    "The results are shown using the \"cummulative regret\" metric, which represents the loss of reccomandations caused by the explore-exploit dilemma, lower is better. \n",
    "It is shown by the authors that the \"etc-seg\" algorithms follow the same curve as \"random\" until the items have been sampled enough, but this causes them to accumulate too much regret. \n",
    "\n",
    "Overall, the best results are clearly shown by ts-seg (which uses clustering instead of personalized contextual vectors).\n",
    "Finally, the authors show a relative display-to-stream rate of an online experiment on the Deezer platform, obtaining a 20% relative increase using ts-seg with their cascade methodology.\n",
    "\n",
    "## Comments\n",
    "\n",
    "First of all, this article does not compare a RL/bandit based system vs other systems. It's a comparison of RL/bandit algorithms using a very specific metric (accumulated regret) that takes on the explore-exploit dilemma. The biggest point to consider in this article, I believe it's the cluster vs personalized contextual vectors. Usually one would believe that having a personalized rating for each user would give better results (particullarly when one uses a reinforcement learning based algorithm), but having a cluster based algorithm have better result, show that either a simple RL algorithm it's not good enough yet, or the contextual vectors where not informative enough."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}