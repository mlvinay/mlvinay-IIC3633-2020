{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# L11-1 Deep learning based recommender system: A survey and New perspective - Part 2\n",
    "## Abstract\n",
    "The later half of the article written by Zhang S. et al. presents another set of deep learning architectural models, such as Recurrent Neural Networks (RNN), Deep Reinforced Learning (DRL), Restricted Blotzmann Machine (RBM), Neural Autoregressive models (NADE), Generative Adersarial Networks (GAN). Furthermore, two techniques that can be impleneted in conjuction with all these models are presented, Neural Attention and Deep hybrid models.\n",
    "\n",
    "Finally, the article presents future directions and open issues, presenting problems common to RS such as joint user-item representation, cross domain recommendation, evaluation metrics and problems. There are also some more deep-learning issues such as explainability, deepness, machine reasoning, deep multi-task learning, scalability.\n",
    "\n",
    "## Comments\n",
    "In respect to the models/techniques presented in this later half, my personal opinion is that RNN present a key function in deep recommender systems, as was stated by the authors, these models have the power to represent sequential information. This quality has two advantages over the rest in particular, the first is... well, retrieving information from naturally sequential items (such as page browsing, music playlist, etc). The second advantage, is it's flexibility on the input size. RNN are capable of ignoring predefined input sizes since one just concatenates as much blocks as necessary, and LSTM blocks keep the long term memory active. This advtange takes place in on-line training, since one can train, even when the user it's active, allowing for more dynamic recommendations.\n",
    "\n",
    "In respect to the future directions and open issues, first I'd like to comment on the authors attachment to attention mechanism, since in about 3 of the sections presented in the fourth section, attention is brought about instead of other mechanisms. For example, in the explainability of reccomendations section, while it's true that attention helps in understanding the hidden layers of a deep-learning network, there are other methods.\n",
    "\n",
    "Finally, in respect to the evaluation issues, using deep-learning models doesn't let researches escape the fact that evualiating RS's is naturally hard. In comparison to common deep-learning classifications, embeddings and other problems, it's quite crucial which metrics does one use, how does one create the test set, what is our comparison baseline, etc. As the authors pointed out, creating a good evaluation framework/guidelines for deep recommender systems it's a crucial issue.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}