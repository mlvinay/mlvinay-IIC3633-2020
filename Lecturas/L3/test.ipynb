{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598806586910",
   "display_name": "Python 3.7.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "The article written by Cremonesi P., Koren Y., and Turrin R. bring two inputs to the sysrec knowledge.\n",
    "The first it's a novel (at such time) evaluation method for sysrec algorithms, that instead of using RMS Error, they take a more binary aproach using recall and precision (a hit or miss aproach).\n",
    "Secondly, they present a \"new\" model, PureSVD. It's not exactly new as it takes the basic SVD formulation $U \\cdot \\Sigma \\cdot Q^T$ and expands it in order to generate a $\\hat{r}_{ui}$ user-item rating.\n",
    "\n",
    "# The evaluation method\n",
    "Considering the existance of a training set $M$, and a probe set $T^{'}$, one first extracts all 5-star ratings from $T^{'}$ in order to create the test set $T$ (this is assuming a clasic 1-5 stars movie rating system). Once the model is trained over $M$, one proceeds for rating of item $i$ in by user $u$ in $T$:\n",
    "\n",
    "1. Select another random 1000 items unrated by $u$, asssuming that they are of uninterest for $u$\n",
    "2. Predict all 1001 ratings \n",
    "3. Order them\n",
    "4. Select the Top-N predicted ratings. If $i$ it's on the Top-N it's a hit. If it's not, it's a miss.\n",
    "\n",
    "Then the evaluation metrics are:\n",
    "\n",
    "- $recall(N) = \\frac{\\#hits}{|T|}$\n",
    "- $precision = \\frac{recall(N)}{N}$\n",
    "\n",
    "# About the evaluation method\n",
    "From here on it's the author's mind speaking. First of all, it's a good evaluation method, changing from a RMSE to a hit or miss idea. In the results it's showed that using a N=10, the best result achieved with various models it's a recall of 0.5, which means that if one where to recommend 10 movies to a user, 1 out of 2 times a movie that the user will probably rate as 5-star will appear, which shows that the models/algorithms are still lacking in delivering almost perfect recomendations. \n",
    "\n",
    "But, using only 5-stars to check for a hit or miss could possibly, be a mistake on evaluation. Can one objectively argue that a 4-star rating is a bad rating? How would the recall have changed if one included 4-star ratings? Making a wild guess, the results would have breaked the 0.9 recall value.\n",
    "\n",
    "On another matter, the precision metric defined by the authors it's a bit problematic on it's naming. In machine learning it's common to use precision as True Positive over Predicted Positive ratio. While the recall defined here follow it's original idea, this defined precision isn't close at all. Furthermore the value presenteded it's diminished by using a bigger N in the top-N ratings, which is counterintuitive as usual machine learning algorithms have an increase in precision when the N increments\n"
   ]
  }
 ]
}