{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# L9-1 : Multi-armed recommender system bandit ensembles\n",
    "## Abstract\n",
    "The article written by Ca√±amares R., Redondo M., and Castells P. presents an implementation of a multi-armed bandit ensemble, and compares said implementation in a off-line experiment that simulates an real case scenario with a baseline hybrid ensemble.\n",
    "\n",
    "The implemented system it's in short words, a reinforcement learning algorithm. Using three different reccomendation algorithms as it's arms (KNN, Matrix factorization and most popular), the system learns to choose which arm to use according to a reward mechanic: $reward = 1$ if the user is pleased with the reccomendation, 0 if it's not. Since this case tries to simulate a real case scenario where every day (or second) there's more information, the authors take 5% of the data set as the initial data, and then update using 1 rating per user, from which the \"test\" itself it's one reccomendation to one user. Every cycle the arms (algorithms) are updated.\n",
    "\n",
    "Two bandit algorithms are tested, $\\epsilon$-greedy and Thompson sampling. $\\epsilon$ -greedy takes the best algorithm (best = biggest accumulated reward) with a $1 - \\epsilon$ chance, or a random algorithm with a $\\epsilon $ chance. Thompson sampling uses a dynamic probability distribution for every arm which it's updated with the rewards. On every cycle, one takes a value from each distribution and the biggest value marks the selected algorithm.\n",
    "\n",
    "This bandit ensemble is then tested against a common hybrid ensemble (of not specified characteristics) on the the MovieLens dataset. Once again, using only 5% of the information as the initial data (about ~10 ratings per user), and using 1 rating per user as a cycle. The results are presented in accumulated recall, where clearly, the best scores are presented by the bandit-ensembles.\n",
    "\n",
    "## Comments\n",
    "About the paper/experiment itself:\n",
    "- While the bandit ensemble it's clearly described, the baseline hybrid ensemble it's just mentioned and used. There's a section on the paper (Section 4.5) that talks about the shortcomings of the hybrid ensemble, which while may be true, it's kind of ironic to say the least since the authors did not present according to which algorithm/function does the baseline ensemble choose which algorithm to use.\n",
    "\n",
    "- The algorithms selected are probably, the three simplest recomendation algorithms there are. While this may be actually a good thing since the objective of this article is to compare ensembles and not algorithms itself, one cannot help but wonder about the possible results of more advanced/complex/stricly better algorithms.\n",
    "\n",
    "- Causality: There's no mention of keeping causality in check. That means using the data from movieLens in the correct time order. While it's understandable to use the data in cycles, if one's intention is to simulate a real case scenario, I believe it to be of utmost importance to keep the rating order in check (as it's probably the same order that the user consumed items).\n",
    "\n",
    "About the theme/ bandits:\n",
    "- Multi-Armed Bandit it's a classic reinforcement learning, where one has limited resources/actions, and multiple choices (arms). This brings about a common theme in RL, that is the exploration-exploitation tradeoff (Explore new choices vs Exploit my actual best choice). In respect to this theme, is it correct in a real case scenario, to not always present the actual best recomendation to a user (which clearly doesnt let the algorithm evolve), or is it okay to send a recomendation from a random algorithm to the user (which seems to beat the point of SysRecs)?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}