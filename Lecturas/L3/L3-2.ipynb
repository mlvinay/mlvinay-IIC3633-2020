{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This lengthy article written by Shani G.  and Gunawardana A. takes a different approach to common papers, as instead of creating or improving over a system, they take make something similar to guidelines in the process of evaluating recommendation systems (which at this point, we do understand that's it's not an easy task).\n",
    "\n",
    "# Experimental settings\n",
    "The authors present three mayor experimental settings which are:\n",
    "\n",
    "1.  Offline Experiments: By far the most common setup, as the only thing required it's a dataset (for example the Netflix dataset), in which one takes a part of the information for training, and another part for testing (there's also the attempt on simulating user actions after training, which the authors delve into in quite depth). \n",
    "\n",
    "2. User Studies: This approach is comparitively more time consuming as there's a need of recruiting test subjects, which interact with the recommendation system in order to gain feedback of it.\n",
    "\n",
    "3. Online Evaluation: Probably the most 'realistic' way of testing a model/algorithm, as it's quite similar to putting said model/algorith to use. Naturally, in order to compare various models at the same time, it's necessary to deploy all of them a the same time in order to create a similar base behavior (which is also delved into by the authors)\n",
    "\n",
    "# Properties\n",
    "\n",
    "The authors present various properties of a recommendation system that could be compared, going from the most common metric, the prediction accuracy (on Rating prediction, Usage Prediction and Ranking Measure), to some up-until-now unheard properties such as Serendipity (How suprising are the recommendations).\n",
    "\n",
    "# Comments\n",
    "As it was mentioned in the abstract, this article's approach is different, as instead of setting a baseline study and pushing it farther, it takes examples from studies and attempts to build a guideline. On another hand, this kind of article could make an even greater contribution to the sysrec community than a normal article, since it attempts to increase the quality of the results (or their presentation) of following articles.\n"
   ]
  }
 ]
}